{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-nearest neighbors\n",
    "\n",
    "This dataset was obtained from https://archive.ics.uci.edu/ml/datasets/Heart+Disease (this is a great resource for datasets to try machine learning on). It has data on patients that are and are not diagnosed with heart disease.\n",
    "\n",
    "The attributes are:\n",
    "* age: age in years \n",
    "* sex: sex (1 = male; 0 = female) \n",
    "* cp: chest pain type \n",
    " * -- Value 1: typical angina \n",
    " * -- Value 2: atypical angina \n",
    " * -- Value 3: non-anginal pain \n",
    " * -- Value 4: asymptomatic \n",
    "* trestbps: resting blood pressure (in mm Hg on admission to the hospital) \n",
    "* chol: serum cholestoral in mg/dl \n",
    "* fbs: (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false) \n",
    "* restecg: resting electrocardiographic results \n",
    " * -- Value 0: normal \n",
    " * -- Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV) \n",
    " * -- Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria \n",
    "* thalach: maximum heart rate achieved \n",
    "* exang: exercise induced angina (1 = yes; 0 = no) \n",
    "* oldpeak = ST depression induced by exercise relative to rest \n",
    "* slope: the slope of the peak exercise ST segment \n",
    " * -- Value 1: upsloping \n",
    " * -- Value 2: flat \n",
    " * -- Value 3: downsloping \n",
    "* ca: number of major vessels (0-3) colored by flourosopy \n",
    "* thal: 3 = normal; 6 = fixed defect; 7 = reversable defect \n",
    "* num: diagnosis of heart disease (angiographic disease status) \n",
    " * -- Value 0: absence.\n",
    " * -- Value 1,2,3,4: presence of heart disease\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the data\n",
    "\n",
    "Read in the data, modify the dependent variable name and plot a histogram of the ages of patients, both healthy and those with heart disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cleveland.csv')\n",
    "\n",
    "# Collapse all values 1-4 into a single value so that \"num\" is boolean\n",
    "df = df.rename({'num':'disease'}, axis=1)\n",
    "df['disease'] = df.disease.apply(lambda x: min(x, 1))\n",
    "df.drop(df[df['thal'] == '?'].index, inplace=True)\n",
    "display(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multiple dimensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knnPredict(data, attributes, numOfNeighbors, target='disease'):\n",
    "    nn = NearestNeighbors(n_neighbors=numOfNeighbors, metric='euclidean', algorithm='auto')\n",
    "    \n",
    "    # Standardize\n",
    "    newAttributes = []\n",
    "    for attribute in attributes:\n",
    "        standard = data[attribute]-data[attribute].mean()/data[attribute].std()\n",
    "        standard.name = standard.name+'_s'\n",
    "        newAttributes.append(standard.name)\n",
    "        data = pd.concat([data,standard],axis=1)\n",
    "        \n",
    "    # Build underlying structure with standardized data   \n",
    "    X = data[newAttributes].values\n",
    "    y = data[target].values\n",
    "    fit = nn.fit(X)\n",
    "    \n",
    "    # Choose a random patient\n",
    "    i = random.randint(0,len(X)-1)\n",
    "    patientX = X[i]\n",
    "    patienty = y[i]\n",
    "    # display('Our patient',data.iloc[i])\n",
    "    \n",
    "    # Find the k nearest neighbors, not including self\n",
    "    distances, indices = fit.kneighbors([patientX],numOfNeighbors+1)\n",
    "    nbrs = data.iloc[indices[0]]\n",
    "    # Delete self from data\n",
    "    nbrs = nbrs.drop(data.iloc[i].name)\n",
    "    # display(nbrs)\n",
    "    \n",
    "    # Count the number of neighbors that have target\n",
    "    have = nbrs[nbrs[target] == 0].count()[target]\n",
    "    # Count the number of neighbors that DON'T have target\n",
    "    dontHave = nbrs[nbrs[target] == 1].count()[target]\n",
    "    # print('Have: {}\\ndontHave: {}'.format(have, dontHave))\n",
    "    \n",
    "    # Predict that our random patient is like the majority of its neighbors\n",
    "    predict = 0 if (dontHave > have) else 1\n",
    "    # According to the records, did our patient have the target\n",
    "    actual = 0 if (patienty == 0) else 1\n",
    "    success = predict == actual\n",
    "    # print('Sucess:',success)\n",
    "    return success, patienty == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testMeSenpai(data, attributes, numOfNeighbors, numOfTests=1, target='disease'):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    for test in range(numOfTests):\n",
    "        yPred, yAct = knnPredict(data, attributes, numOfNeighbors)\n",
    "        y_pred.append(yPred)\n",
    "        y_true.append(yAct)\n",
    "    return y_pred ,y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossValidate(data, attributes, numOfNeighbors, numOfTests, target='disease', level = 10):\n",
    "    folds = []\n",
    "    for i in range(level):\n",
    "        folds.append(data.sample(frac = 1/level))\n",
    "        \n",
    "    # Train missing one fold\n",
    "    for i in range(level-1):\n",
    "        temp = pd.concat([data,folds[i]])\n",
    "        temp = temp.drop_duplicates(keep=False)\n",
    "        \n",
    "        y_pred,y_true = testMeSenpai(temp, attributes, numOfNeighbors, numOfTests, target='disease')\n",
    "        (p,r,f,s) = precision_recall_fscore_support(y_pred,y_true,zero_division=0)\n",
    "        print(p,r,f,s,'\\n')\n",
    "        \n",
    "df1 = df.copy()\n",
    "k = 5\n",
    "numOfTests = 5\n",
    "attributes = ['age','trestbps']\n",
    "\n",
    "crossValidate(df1,attributes,k,numOfTests)\n",
    "# y_pred, y_true = testMeSenpai(df1,attributes,k,numOfTests)\n",
    "# (p,r,f,s) = precision_recall_fscore_support(y_pred,y_true)\n",
    "# display(p,r,f,s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testAgainstTest(testData,attributes,k):\n",
    "    y_pred, y_true = testMeSenpai(testData,attributes,k)\n",
    "    (p,r,f,s) = precision_recall_fscore_support(y_pred,y_true)\n",
    "    return p,r,f,s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is the value of k (nearest neigbors) and set of attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day of the challenge set, all we need to do is change this file name and run it\n",
    "new_df = pd.read_csv('cleveland-test-sample.csv')\n",
    "\n",
    "# Collapse all values 1-4 into a single value so that \"num\" is boolean\n",
    "new_df = new_df.rename({'num':'disease','Unnamed: 0':'id'}, axis=1)\n",
    "new_df['disease'] = new_df.disease.apply(lambda x: min(x, 1))\n",
    "\n",
    "# Build a bigger data set to \n",
    "temp = pd.concat([df,new_df]).reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chosen features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "chosen_features = ['cp', 'trestbps', 'chol', 'restecg', 'thalach', 'exang', 'thal', 'age']\n",
    "numFeatures = 5\n",
    "attributes = ['age','trestbps','chol', 'thalach']\n",
    "# standardizing noncatagorical data\n",
    "newAttributes = []\n",
    "data1 = df.copy()\n",
    "for attribute in attributes:\n",
    "    standard = (data1[attribute]-data1[attribute].mean())/data1[attribute].std()\n",
    "    chosen_features.remove(standard.name)\n",
    "    chosen_features.append(standard.name+'_s')\n",
    "    standard.name = standard.name+'_s'\n",
    "    newAttributes.append(standard.name)\n",
    "    data1 = pd.concat([data1,standard],axis=1)\n",
    "    data1.drop(attribute,axis=1,inplace=True)\n",
    "def makefeaturesList(chosen_features, n):\n",
    "    featureslist = list(itertools.combinations(chosen_features,n)) #\n",
    "    featureslist = [list(x) for x in featureslist]\n",
    "    return featureslist\n",
    "# data 1 is processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## monte carlo data split with selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make random split for one monte carlo cross validation run\n",
    "def monteCarloCVSplit(data : pd.DataFrame , level = 10):\n",
    "    CVtest = data.sample(frac = 1/level)\n",
    "    CVtrain = data.drop(CVtest.index)\n",
    "    return CVtrain, CVtest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run knn on given attributes and CV split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knnPredictions(CVtrain, CVTest, attributes, k, target='disease'):\n",
    "    nn = NearestNeighbors(n_neighbors=k , metric='euclidean', algorithm='auto')\n",
    "    \n",
    "    # Build underlying structure with standardized data   \n",
    "    X = CVtrain[attributes].values\n",
    "    y = CVtrain[target].values\n",
    "    \n",
    "    testX = CVTest[attributes].values\n",
    "    testy = CVTest[target].values\n",
    "    fit = nn.fit(X) # fits training data to model\n",
    "    \n",
    "    # nbrs = neighbors\n",
    "    #this fit finds the nearest neigbors from the training set to the test set \n",
    "    distances, nbrs = fit.kneighbors(testX, n_neighbors = k) \n",
    "    \n",
    "    # Count the number of neighbors that have target\n",
    "\n",
    "    diseasdedNbrCount = []\n",
    "    for nbrList in nbrs:\n",
    "        diseasdedNbrCount.append([1 if y[nbr] == 1 else 0 for nbr in nbrList])\n",
    "    \n",
    "    predictions = []\n",
    "    for l in diseasdedNbrCount:\n",
    "        numOfDiseased = sum(l)\n",
    "        if numOfDiseased > k/2:\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "    # print(predictions,'\\n',diseasdedNbrCount)\n",
    "    return predictions , testy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CVtrain1, CVtest1 = monteCarloCVSplit(data1)\n",
    "featureslist = makefeaturesList(chosen_features, 7)\n",
    "preds, actuals = knnPredictions(CVtrain1, CVtest1, featureslist[0] , 5)\n",
    "\n",
    "(p,r,f,s) = precision_recall_fscore_support(preds,actuals,zero_division=0,average='binary')\n",
    "f1 = (p*r)/(p + r)\n",
    "print(p,r,f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traffic = pd.read_csv('traffic-Brazil.csv',sep=';')\n",
    "# traffic = traffic.rename(columns={'Slowness in traffic (%)':'slowness'})\n",
    "# traffic['slowness'] = traffic['slowness'].str.replace(',','.').astype(float)\n",
    "# trafficMean = traffic['slowness'].mean()\n",
    "# traffic['highTraffic'] = traffic['slowness'] > trafficMean\n",
    "\n",
    "# attributes = ['Immobilized bus', 'Broken Truck','Vehicle excess','Accident victim']\n",
    "# k = 6\n",
    "# target = \n",
    "\n",
    "power = pd.read_csv('city_power_consumption.csv')\n",
    "power = power.rename(columns={'Zone 1 Power Consumption':'Zone_1_Power'})\n",
    "powerMean = power['Zone_1_Power'].mean()\n",
    "power['highPowerUse'] = power.Zone_1_Power>powerMean\n",
    "# power.plot(power['Temperature'])\n",
    "# sns.lineplot(data = power, x = 'Temperature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = ['Temperature','Humidity','Wind Speed','Zone 2  Power Consumption']\n",
    "k = 50\n",
    "target = 'highPowerUse'\n",
    "# display(power)\n",
    "# display(renameMe(power,attributes,k,target = target))\n",
    "# display(testMeSenpai(power,attributes,k,numOfTests = 10,target = target))\n",
    "# crossValidate(power,attributes,k,numOfTests,target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data = power.sample(frac=.01), x = 'Temperature', y = 'Zone_1_Power')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data = power.sample(frac=.01), x = 'Humidity', y = 'Zone_1_Power')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day of the challenge set, all we need to do is change this file name and run it\n",
    "new_df = pd.read_csv('cleveland-test-sample.csv')\n",
    "\n",
    "# Collapse all values 1-4 into a single value so that \"num\" is boolean\n",
    "new_df = new_df.rename({'num':'disease','Unnamed: 0':'id'}, axis=1)\n",
    "new_df['disease'] = new_df.disease.apply(lambda x: min(x, 1))\n",
    "\n",
    "# Build a bigger data set to \n",
    "temp = pd.concat([df,new_df]).reset_index()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "8112dd241fa2daf28d9e3e7428f74cc036918cdd417492de7d9a77b623e05e74"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
